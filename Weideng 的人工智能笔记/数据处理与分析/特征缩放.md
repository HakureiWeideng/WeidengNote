# 特征缩放（Feature scaling）

作用如下：

- 使不同量纲的特征处于同一数值量级，减少方差大的特征的影响，使模型更准确。

- 加快学习算法的收敛速度。

## 概念区分

### 标准化（standardization）

使得不同度量之间的特征具有可比性，同时不改变原始数据的分布。

**将数据变成均值为 0，方差为 1，但仍服从原来的分布。**

好处：

- 使得不同度量之间的特征具有可比性，对目标函数的影响体现在几何分布上，而不是数值上。

- 不改变原始数据的分布。

### 归一化（normalization）

使各个特征维度对目标函数的影响权重是一致的，即使得那些扁平分布的数据伸缩变换成类圆形，**这也就改变了原始数据的分布**。

**将数据变为 0 到 1 。**

好处：

- 提高迭代求解的收敛速度。
- 提高迭代求解的精度。

## 四种特征缩放

### Rescaling 

(min-max normalization，即 归一化) 有时简称 normalization

$$x' = {x - \min(x) \over \max(x)-\min(x)} $$

### Mean normalization

$$x' = {x - mean(x) \over \max(x)-\min(x)} $$

### Standardization

(Z-score normalization，即 标准化)

$$x' = {x-mean(x)\over \sigma}$$

### Scaling to unit length

$$x' = {x \over ||x||}$$