# 最优化方法（Optimization Method）

使损失函数最小化的方法。

## 梯度下降（Gradient Descent）

函数在可微点沿负梯度方向下降最多。

- 局部最优解（Local Optimum）局部最小值（Local minimum）（minimum 的复数为 minima）

- 全局最优解（Global Optimum）全局最小值（Global minimum）

- 梯度上升（Gradient Increase）

	将下降问题转化为上升问题。

### 基本算法

1. 将参数表示为一个向量 $\boldsymbol \theta$ 。

2. 随机选取一个初始值 $\boldsymbol\theta^0$ 。也可随机选取多个初始值，多做几次。

3. 对全部参数求偏导 $\boldsymbol g = \nabla L(\boldsymbol \theta^i),\boldsymbol \theta^{i+1} = \boldsymbol \theta^i - \eta \boldsymbol g$ ，学习率 $\eta$ 可控制下降的跨度。

4. 不断重复迭代，直到设定次数停止或结果无法继续求偏导。

注意，不是对输入参数求偏导，而是对参数求偏导，输入参数，输出参数和标签的值将被代入到偏导中，以计算出偏导的具体值，从而用于更新权值。

### 训练过程

整个神经网络可以看做一个多层**嵌套函数**，在神经网络中一个样本的训练过程：

- 前向传播（Forward Propagation / Forward Pass）

  神经网络中整个从输入参数得到输出参数的计算过程。

  - 神经网络的整体前向传播的计算过程是嵌套函数中**由内而外**的。
  - 越接近输出参数，就越外层。

- 反向传播（Backpropagation / Backward Pass，BP 算法）

  使用梯度下降更新参数的过程。

  - 损失函数的参数为神经网络的输出参数，因此**损失函数是对神经网络嵌套函数的外加一层嵌套**。
  - 需要使用**链式法则（Chain Rule）**求**由外而内**求**损失函数**对参数的偏导。

  - 神经网络的参数连接形式与链式法则中变量依赖关系形式一致，可以把各层偏导结果存储下来，逐层更新参数。

### 训练方式

- 批量梯度下降法（Batch Gradient Descent，BGD）

	每次更新使用所有样本的参数。

- 随机梯度下降法（Stochastic Gradient Descent，SGD）

	每次更新随机使用一个样本的参数。

- 小批量梯度下降法（Mini-batch Gradient Descent，MBGD）

	每次更新使用一个批次的样本的参数。

![image-20220603192637634](images/最优化方法/image-20220603192637634.png)

- 批次（Batch）

  将样本分批次输入神经网络进行训练，更新参数，以加速计算过程。

- Epoch

	将所有样本都分批训练完，即将训练集使用完一次，称为一个 epoch 。

- Iteration

	一个 epoch 含有的 batch 数目。

一个批次的大小（Batch Size）和进行 epoch 的次数常做为超参数。

一个样本包含神经网络的所有输入参数，一个样本的训练包含前向传播和反向传播两个过程。

在神经网络的训练过程中，训练集会被利用多次，即同一个样本会被多次输入神经网络进行训练，次数取决于 epoch 。
